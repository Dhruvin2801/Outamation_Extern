{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1bL5ZgjL8TkDV9kJ2GQ1L9eHaqck7EKfZ","timestamp":1757999081247},{"file_id":"1S0AuOQW2JsrXKnU6rGKLRzhOfMyGptBK","timestamp":1754428503769},{"file_id":"19C_a22OssmdBR86hx6gE_1viuqA7OkAV","timestamp":1754285734935}]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"c3aa2c6b6f7841989afb6a100b952afd":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cc4d50a95b3f43e782f886dd5146aaf1","IPY_MODEL_98057286293d42c9864daf18481f883c","IPY_MODEL_d965fba23d3b44c992bdc3984749a7d2"],"layout":"IPY_MODEL_3ce32b6cca37487580f4bc7e72df467e"}},"cc4d50a95b3f43e782f886dd5146aaf1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e595c8e8ef94f7385d09fa42459817b","placeholder":"​","style":"IPY_MODEL_97e38358ebb14978a1bf137acf408598","value":"Batches: 100%"}},"98057286293d42c9864daf18481f883c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_75e48517bae846c3bd1aab5f45adbf31","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_65f13cdcd8684079b5ac77bbf2f6c51f","value":1}},"d965fba23d3b44c992bdc3984749a7d2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_09a806104a6f4409a9387c44d7ec4005","placeholder":"​","style":"IPY_MODEL_776db3a8e2c94292886fa8593e82820f","value":" 1/1 [00:00&lt;00:00,  1.87it/s]"}},"3ce32b6cca37487580f4bc7e72df467e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e595c8e8ef94f7385d09fa42459817b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97e38358ebb14978a1bf137acf408598":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"75e48517bae846c3bd1aab5f45adbf31":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"65f13cdcd8684079b5ac77bbf2f6c51f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"09a806104a6f4409a9387c44d7ec4005":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"776db3a8e2c94292886fa8593e82820f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"71929f3f2bff40469af3cced1a3682ea":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c911b2c5b0874d0d905d50122e7fe5d6","IPY_MODEL_49a6916f7e9b4a8a80217e4b91752e5e","IPY_MODEL_d6965a0eceb24f17a860449cf55562ec"],"layout":"IPY_MODEL_d6c4825c9e0e4a4c926b4b5cc4c51bc7"}},"c911b2c5b0874d0d905d50122e7fe5d6":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f59d4a8273e543ed81ae95a84f2510ff","placeholder":"​","style":"IPY_MODEL_205cb200b0e24214a09d218ffdf7760f","value":"Batches: 100%"}},"49a6916f7e9b4a8a80217e4b91752e5e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2eb534d1e5b84192b496c89f3ad027b7","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e3b02ebba16742879d2309d29f66599b","value":1}},"d6965a0eceb24f17a860449cf55562ec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca62e0e2d37a48b9af2bdc46474d84cf","placeholder":"​","style":"IPY_MODEL_0626dfabfdff46c7851af8d2f8b24911","value":" 1/1 [00:00&lt;00:00,  1.08it/s]"}},"d6c4825c9e0e4a4c926b4b5cc4c51bc7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f59d4a8273e543ed81ae95a84f2510ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"205cb200b0e24214a09d218ffdf7760f":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2eb534d1e5b84192b496c89f3ad027b7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3b02ebba16742879d2309d29f66599b":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ca62e0e2d37a48b9af2bdc46474d84cf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0626dfabfdff46c7851af8d2f8b24911":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"91f360da928b49c7bedca23bc5a8ac5b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fa089194963b4189b5eb0252a4bfa7f8","IPY_MODEL_8ce86fdcf2724c32a01f9248402b4bdb","IPY_MODEL_718dde41bfb447868427ac9cc91dd13c"],"layout":"IPY_MODEL_36cc5c055bc34dc1adbab662cd0b814d"}},"fa089194963b4189b5eb0252a4bfa7f8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_92273b9a5eba4677bc0431c034903b01","placeholder":"​","style":"IPY_MODEL_68aea2b85a164fdea012fdc3d3313a21","value":"Batches: 100%"}},"8ce86fdcf2724c32a01f9248402b4bdb":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1ba23b9983e849a498027a6e551f43a4","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_523f5f5435594ebd9e8468de2ed4eb34","value":1}},"718dde41bfb447868427ac9cc91dd13c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_dc3293f05c11409992eaa7add37f5d5c","placeholder":"​","style":"IPY_MODEL_4b1a05dfb0944507be4d4a2fc7d7688c","value":" 1/1 [00:00&lt;00:00,  1.93it/s]"}},"36cc5c055bc34dc1adbab662cd0b814d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92273b9a5eba4677bc0431c034903b01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68aea2b85a164fdea012fdc3d3313a21":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1ba23b9983e849a498027a6e551f43a4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"523f5f5435594ebd9e8468de2ed4eb34":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"dc3293f05c11409992eaa7add37f5d5c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b1a05dfb0944507be4d4a2fc7d7688c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Combining Multi-Document Intelligence with User-Friendly Interface\n","\n","This notebook demonstrates how to build a production-ready document Q&A system that combines:\n","\n","- Intelligent multi-document detection and classification\n","- Query routing for improved retrieval accuracy\n","- Rich metadata preservation\n","- User-friendly Gradio interface"],"metadata":{"id":"Oh6V_eigIL1i"}},{"cell_type":"markdown","source":["## 📚 Setup and Installation\n","First, let's install all necessary packages"],"metadata":{"id":"riBw_f9-IY4T"}},{"cell_type":"code","source":["# Install required packages\n","!pip install -q gradio\n","!pip install -q gradio_pdf\n","!pip install -q pypdf PyPDF2 pymupdf\n","!pip install -q sentence-transformers transformers\n","!pip install -q faiss-cpu\n","!pip install -q google-generativeai\n","!pip install -q numpy pandas\n","\n","# Install LlamaIndex packages for enhanced document processing\n","!pip install -q llama-index\n","!pip install -q llama-index-readers-file\n","!pip install -q llama-index-embeddings-huggingface\n","!pip install -q llama-index-vector-stores-faiss\n","!pip install -q llama-index-llms-gemini"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MEXBca59DFk2","executionInfo":{"status":"ok","timestamp":1754429779786,"user_tz":240,"elapsed":284510,"user":{"displayName":"Chaitanya Baweja","userId":"09943888125622581847"}},"outputId":"543afc83-2320-4135-b58a-8c1d46dfe31b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.2/313.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m26.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m44.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.1/137.1 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"markdown","source":["## 🔧 Core Imports and Configuration"],"metadata":{"id":"_0qPyDllJbDH"}},{"cell_type":"code","source":["import gradio as gr\n","from gradio_pdf import PDF\n","import fitz  # PyMuPDF\n","from PyPDF2 import PdfReader\n","import numpy as np\n","from sentence_transformers import SentenceTransformer\n","import faiss\n","import google.generativeai as genai\n","from typing import List, Dict, Tuple, Optional\n","from dataclasses import dataclass\n","import json\n","from datetime import datetime\n","import hashlib\n","\n","# LlamaIndex imports for enhanced document processing\n","from llama_index.core import Document, VectorStoreIndex, StorageContext\n","from llama_index.core.schema import TextNode\n","from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n","from llama_index.core.node_parser import SentenceSplitter\n","from llama_index.core.vector_stores import MetadataFilters, MetadataFilter, FilterOperator\n","\n","# Configure Gemini (replace with your API key)\n","GEMINI_API_KEY = \"AIzaSyBUZXaw8UOeWE5h8e6sSMKv3kA4H4H3NiQ\"\n","genai.configure(api_key=GEMINI_API_KEY)\n","gemini_model = genai.GenerativeModel(\"models/gemini-2.0-flash\")\n","\n","# Initialize embedding models (both for compatibility)\n","embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n","llama_embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"],"metadata":{"id":"N_QN4jpxJcGt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 📄 Data Structures for Enhanced Document Management\n","Let's define our data structures to handle complex document metadata:"],"metadata":{"id":"J9FtUNVZJjnz"}},{"cell_type":"code","source":["@dataclass\n","class PageInfo:\n","    \"\"\"Stores information about a single page\"\"\"\n","    page_num: int\n","    text: str\n","    doc_type: Optional[str] = None\n","    page_in_doc: int = 0\n","\n","@dataclass\n","class LogicalDocument:\n","    \"\"\"Represents a logical document within a PDF\"\"\"\n","    doc_id: str\n","    doc_type: str\n","    page_start: int\n","    page_end: int\n","    text: str\n","    chunks: List[Dict] = None\n","\n","@dataclass\n","class ChunkMetadata:\n","    \"\"\"Rich metadata for each chunk\"\"\"\n","    chunk_id: str\n","    doc_id: str\n","    doc_type: str\n","    chunk_index: int\n","    page_start: int\n","    page_end: int\n","    text: str\n","    embedding: Optional[np.ndarray] = None"],"metadata":{"id":"NiN5Ydr1Jpio"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 🧠 Document Intelligence Functions\n","These functions handle document classification and boundary detection:"],"metadata":{"id":"rMN46qYfJrz9"}},{"cell_type":"code","source":["def classify_document_type(text: str, max_length: int = 1500) -> str:\n","    \"\"\"\n","    Classify the document type based on its content.\n","    Uses LLM to intelligently identify document category.\n","    \"\"\"\n","    # Truncate text if too long to avoid token limits\n","    text_sample = text[:max_length] if len(text) > max_length else text\n","\n","    prompt = f\"\"\"\n","    Analyze this document and classify it into ONE of these categories:\n","    - Resume: CV, professional profile, work history\n","    - Contract: Legal agreement, terms and conditions, service agreement\n","    - Mortgage Contract: Home loan agreement, mortgage terms, property financing\n","    - Invoice: Bill, payment request, financial statement\n","    - Pay Slip: Salary statement, wage slip, earnings statement\n","    - Lender Fee Sheet: Loan fees, lender charges, closing costs\n","    - Land Deed: Property deed, title document, ownership certificate\n","    - Bank Statement: Account statement, transaction history\n","    - Tax Document: W2, 1099, tax return, tax form\n","    - Insurance: Insurance policy, coverage document\n","    - Report: Analysis, research document, findings\n","    - Letter: Correspondence, memo, communication\n","    - Form: Application, questionnaire, data entry form\n","    - ID Document: Driver's license, passport, identification\n","    - Medical: Medical report, prescription, health record\n","    - Other: Doesn't fit other categories\n","\n","    Document sample:\n","    {text_sample}\n","\n","    Respond with ONLY the category name, nothing else.\n","    \"\"\"\n","\n","    try:\n","        response = gemini_model.generate_content(prompt)\n","        doc_type = response.text.strip()\n","\n","        # Normalize the response\n","        valid_types = [\n","            'Resume', 'Contract', 'Mortgage Contract', 'Invoice', 'Pay Slip',\n","            'Lender Fee Sheet', 'Land Deed', 'Bank Statement', 'Tax Document',\n","            'Insurance', 'Report', 'Letter', 'Form', 'ID Document',\n","            'Medical', 'Other'\n","        ]\n","\n","        # Find best match (case-insensitive)\n","        for valid_type in valid_types:\n","            if doc_type.lower() == valid_type.lower():\n","                return valid_type\n","\n","        return 'Other'\n","    except Exception as e:\n","        print(f\"Classification error: {e}\")\n","        return 'Other'\n","\n","def detect_document_boundary(prev_text: str, curr_text: str,\n","                            current_doc_type: str = None) -> bool:\n","    \"\"\"\n","    Detect if two consecutive pages belong to the same document.\n","    Returns True if they're from the same document.\n","    \"\"\"\n","    # Quick heuristic checks first\n","    if not prev_text or not curr_text:\n","        return False\n","\n","    # Sample the texts for LLM analysis\n","    prev_sample = prev_text[-500:] if len(prev_text) > 500 else prev_text\n","    curr_sample = curr_text[:500] if len(curr_text) > 500 else curr_text\n","\n","    prompt = f\"\"\"\n","    Determine if these two pages are from the SAME document.\n","\n","    Current document type: {current_doc_type or 'Unknown'}\n","\n","    End of Previous Page:\n","    ...{prev_sample}\n","\n","    Start of Current Page:\n","    {curr_sample}...\n","\n","    Consider:\n","    - Continuity of content\n","    - Formatting consistency\n","    - Topic coherence\n","    - Page numbers or headers\n","\n","    Answer ONLY 'Yes' if same document or 'No' if different document.\n","    \"\"\"\n","\n","    try:\n","        response = gemini_model.generate_content(prompt)\n","        return response.text.strip().lower().startswith('yes')\n","    except Exception as e:\n","        print(f\"Boundary detection error: {e}\")\n","        # Default to keeping pages together if uncertain\n","        return True"],"metadata":{"id":"TSkSE_xPIZQK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 📑 Advanced PDF Processing Pipeline\n","Now let's build the enhanced PDF processing pipeline:"],"metadata":{"id":"sz_YivbFJ10K"}},{"cell_type":"code","source":["def extract_and_analyze_pdf(pdf_file) -> Tuple[List[PageInfo], List[LogicalDocument]]:\n","    \"\"\"\n","    Extract text from PDF and perform intelligent document analysis.\n","    Returns both page-level info and logical document groupings.\n","    Supports various file types including scanned PDFs with OCR.\n","    \"\"\"\n","    print(\"📖 Starting PDF extraction and analysis...\")\n","\n","    # Extract text from each page\n","    if isinstance(pdf_file, dict) and \"content\" in pdf_file:\n","        doc = fitz.open(stream=pdf_file[\"content\"], filetype=\"pdf\")\n","    elif hasattr(pdf_file, \"read\"):\n","        doc = fitz.open(stream=pdf_file.read(), filetype=\"pdf\")\n","    else:\n","        doc = fitz.open(pdf_file)\n","\n","    pages_info = []\n","    for i, page in enumerate(doc):\n","        text = page.get_text()\n","\n","        # If no text found, try OCR (for scanned documents)\n","        if not text.strip():\n","            print(f\"  Page {i}: No text found, attempting OCR...\")\n","            try:\n","                # Convert page to image and perform OCR\n","                pix = page.get_pixmap()\n","                img_data = pix.tobytes(\"png\")\n","                from PIL import Image\n","                import pytesseract\n","                import io\n","\n","                img = Image.open(io.BytesIO(img_data))\n","                text = pytesseract.image_to_string(img)\n","                print(f\"  Page {i}: OCR extracted {len(text)} characters\")\n","            except Exception as e:\n","                print(f\"  Page {i}: OCR failed - {e}\")\n","                text = \"\"\n","\n","        pages_info.append(PageInfo(page_num=i, text=text))\n","\n","    doc.close()\n","\n","    if not pages_info:\n","        raise ValueError(\"No text could be extracted from PDF\")\n","\n","    print(f\"✅ Extracted {len(pages_info)} pages\")\n","\n","    # Perform document classification and boundary detection\n","    print(\"🧠 Analyzing document structure...\")\n","    logical_docs = []\n","    current_doc_type = None\n","    current_doc_pages = []\n","    doc_counter = 0\n","\n","    for i, page_info in enumerate(pages_info):\n","        if i == 0:\n","            # First page - classify document type\n","            current_doc_type = classify_document_type(page_info.text)\n","            page_info.doc_type = current_doc_type\n","            page_info.page_in_doc = 0\n","            current_doc_pages = [page_info]\n","            print(f\"  Page {i}: New document detected - {current_doc_type}\")\n","        else:\n","            # Check if this page continues the previous document\n","            prev_text = pages_info[i-1].text\n","            is_same = detect_document_boundary(prev_text, page_info.text, current_doc_type)\n","\n","            if is_same:\n","                # Continue current document\n","                page_info.doc_type = current_doc_type\n","                page_info.page_in_doc = len(current_doc_pages)\n","                current_doc_pages.append(page_info)\n","            else:\n","                # New document detected - save previous and start new\n","                logical_doc = LogicalDocument(\n","                    doc_id=f\"doc_{doc_counter}\",\n","                    doc_type=current_doc_type,\n","                    page_start=current_doc_pages[0].page_num,\n","                    page_end=current_doc_pages[-1].page_num,\n","                    text=\"\\n\\n\".join([p.text for p in current_doc_pages])\n","                )\n","                logical_docs.append(logical_doc)\n","                doc_counter += 1\n","\n","                # Start new document\n","                current_doc_type = classify_document_type(page_info.text)\n","                page_info.doc_type = current_doc_type\n","                page_info.page_in_doc = 0\n","                current_doc_pages = [page_info]\n","                print(f\"  Page {i}: New document detected - {current_doc_type}\")\n","\n","    # Don't forget the last document\n","    if current_doc_pages:\n","        logical_doc = LogicalDocument(\n","            doc_id=f\"doc_{doc_counter}\",\n","            doc_type=current_doc_type,\n","            page_start=current_doc_pages[0].page_num,\n","            page_end=current_doc_pages[-1].page_num,\n","            text=\"\\n\\n\".join([p.text for p in current_doc_pages])\n","        )\n","        logical_docs.append(logical_doc)\n","\n","    print(f\"✅ Identified {len(logical_docs)} logical documents\")\n","    for ld in logical_docs:\n","        print(f\"   - {ld.doc_type}: Pages {ld.page_start}-{ld.page_end}\")\n","\n","    return pages_info, logical_docs"],"metadata":{"id":"PIBKFsndJ5TT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ✂️ Intelligent Chunking with Metadata Preservation\n","We'll provide two chunking approaches - our custom implementation and LlamaIndex's built-in capabilities:"],"metadata":{"id":"LM89pqZjK_mK"}},{"cell_type":"code","source":["def chunk_document_with_metadata(logical_doc: LogicalDocument,\n","                                chunk_size: int = 500,\n","                                overlap: int = 100) -> List[ChunkMetadata]:\n","    \"\"\"\n","    Chunk a logical document while preserving rich metadata.\n","    Uses sliding window with overlap for better context.\n","    \"\"\"\n","    chunks_metadata = []\n","    words = logical_doc.text.split()\n","\n","    if len(words) <= chunk_size:\n","        # Document is small enough to be a single chunk\n","        chunk_meta = ChunkMetadata(\n","            chunk_id=f\"{logical_doc.doc_id}_chunk_0\",\n","            doc_id=logical_doc.doc_id,\n","            doc_type=logical_doc.doc_type,\n","            chunk_index=0,\n","            page_start=logical_doc.page_start,\n","            page_end=logical_doc.page_end,\n","            text=logical_doc.text\n","        )\n","        chunks_metadata.append(chunk_meta)\n","    else:\n","        # Create overlapping chunks\n","        stride = chunk_size - overlap\n","        for i, start_idx in enumerate(range(0, len(words), stride)):\n","            end_idx = min(start_idx + chunk_size, len(words))\n","            chunk_text = ' '.join(words[start_idx:end_idx])\n","\n","            # Calculate which pages this chunk spans\n","            # (simplified - in production, track more precisely)\n","            chunk_position = start_idx / len(words)\n","            page_range = logical_doc.page_end - logical_doc.page_start\n","            relative_page = int(chunk_position * page_range)\n","            chunk_page_start = logical_doc.page_start + relative_page\n","            chunk_page_end = min(chunk_page_start + 1, logical_doc.page_end)\n","\n","            chunk_meta = ChunkMetadata(\n","                chunk_id=f\"{logical_doc.doc_id}_chunk_{i}\",\n","                doc_id=logical_doc.doc_id,\n","                doc_type=logical_doc.doc_type,\n","                chunk_index=i,\n","                page_start=chunk_page_start,\n","                page_end=chunk_page_end,\n","                text=chunk_text\n","            )\n","            chunks_metadata.append(chunk_meta)\n","\n","            if end_idx >= len(words):\n","                break\n","\n","    return chunks_metadata\n","\n","def chunk_with_llama_index(logical_doc: LogicalDocument,\n","                           chunk_size: int = 500,\n","                           chunk_overlap: int = 100) -> List[Document]:\n","    \"\"\"\n","    Alternative: Use LlamaIndex's advanced chunking with metadata.\n","    \"\"\"\n","    # Create LlamaIndex document with metadata\n","    doc = Document(\n","        text=logical_doc.text,\n","        metadata={\n","            \"doc_id\": logical_doc.doc_id,\n","            \"doc_type\": logical_doc.doc_type,\n","            \"page_start\": logical_doc.page_start,\n","            \"page_end\": logical_doc.page_end,\n","            \"source\": f\"{logical_doc.doc_type}_document\"\n","        }\n","    )\n","\n","    # Use LlamaIndex's sentence splitter for better chunking\n","    splitter = SentenceSplitter(\n","        chunk_size=chunk_size,\n","        chunk_overlap=chunk_overlap,\n","        paragraph_separator=\"\\n\\n\",\n","        separator=\" \",\n","    )\n","\n","    # Create nodes (chunks) from document\n","    nodes = splitter.get_nodes_from_documents([doc])\n","\n","    # Convert to our ChunkMetadata format for consistency\n","    chunks_metadata = []\n","    for i, node in enumerate(nodes):\n","        chunk_meta = ChunkMetadata(\n","            chunk_id=f\"{logical_doc.doc_id}_chunk_{i}\",\n","            doc_id=logical_doc.doc_id,\n","            doc_type=logical_doc.doc_type,\n","            chunk_index=i,\n","            page_start=node.metadata.get(\"page_start\", logical_doc.page_start),\n","            page_end=node.metadata.get(\"page_end\", logical_doc.page_end),\n","            text=node.text\n","        )\n","        chunks_metadata.append(chunk_meta)\n","\n","    return chunks_metadata\n","\n","def process_all_documents(logical_docs: List[LogicalDocument],\n","                         use_llama_index: bool = False) -> List[ChunkMetadata]:\n","    \"\"\"\n","    Process all logical documents into chunks with metadata.\n","    Can use either custom or LlamaIndex chunking.\n","    \"\"\"\n","    all_chunks = []\n","\n","    for logical_doc in logical_docs:\n","        if use_llama_index:\n","            chunks = chunk_with_llama_index(logical_doc)\n","        else:\n","            chunks = chunk_document_with_metadata(logical_doc)\n","\n","        logical_doc.chunks = chunks  # Store reference\n","        all_chunks.extend(chunks)\n","        print(f\"📄 {logical_doc.doc_type}: Created {len(chunks)} chunks\")\n","\n","    return all_chunks"],"metadata":{"id":"G6jI6IMnLCX1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 🎯 Query Routing and Intelligent Retrieval"],"metadata":{"id":"RESzcztYLEgS"}},{"cell_type":"code","source":["def predict_query_document_type(query: str) -> Tuple[str, float]:\n","    \"\"\"\n","    Predict which document type is most likely to contain the answer.\n","    Returns predicted type and confidence score.\n","    \"\"\"\n","    prompt = f\"\"\"\n","    Analyze this query and predict which document type would most likely contain the answer.\n","\n","    Query: \"{query}\"\n","\n","    Choose the MOST LIKELY type from:\n","    - Resume: Career, experience, education, skills, employment history\n","    - Contract: Terms, agreements, obligations, parties, legal terms\n","    - Mortgage Contract: Home loan, property financing, mortgage terms, interest rates\n","    - Invoice: Payments, amounts due, billing, charges, invoiced items\n","    - Pay Slip: Salary, wages, deductions, earnings, pay period\n","    - Lender Fee Sheet: Loan fees, closing costs, origination fees, lender charges\n","    - Land Deed: Property ownership, deed information, property description, title\n","    - Bank Statement: Account balance, transactions, deposits, withdrawals\n","    - Tax Document: Tax information, W2, 1099, tax returns, tax amounts\n","    - Insurance: Coverage, policy details, premiums, claims\n","    - Report: Analysis, findings, conclusions, research data\n","    - Letter: Communications, requests, notifications, correspondence\n","    - Form: Applications, submitted data, form fields\n","    - ID Document: Personal identification, ID numbers, identity verification\n","    - Medical: Health information, medical conditions, prescriptions\n","    - Other: General or unclear\n","\n","    Respond in JSON format:\n","    {{\"type\": \"DocumentType\", \"confidence\": 0.85}}\n","\n","    Confidence should be between 0.0 and 1.0\n","    \"\"\"\n","\n","    try:\n","        response = gemini_model.generate_content(prompt)\n","        result = json.loads(response.text.strip())\n","        return result.get(\"type\", \"Other\"), result.get(\"confidence\", 0.5)\n","    except Exception as e:\n","        print(f\"Query routing error: {e}\")\n","        return \"Other\", 0.0\n","\n","class IntelligentRetriever:\n","    \"\"\"\n","    Advanced retrieval system with metadata filtering and query routing.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.index = None\n","        self.chunks_metadata = []\n","        self.doc_type_indices = {}  # Separate indices per doc type\n","\n","    def build_indices(self, chunks_metadata: List[ChunkMetadata]):\n","        \"\"\"\n","        Build FAISS indices with document type segregation.\n","        \"\"\"\n","        print(\"🔨 Building vector indices...\")\n","        self.chunks_metadata = chunks_metadata\n","\n","        # Create embeddings for all chunks\n","        texts = [chunk.text for chunk in chunks_metadata]\n","        embeddings = embed_model.encode(texts, show_progress_bar=True)\n","\n","        # Store embeddings in metadata\n","        for i, chunk in enumerate(chunks_metadata):\n","            chunk.embedding = embeddings[i]\n","\n","        # Build main index\n","        dim = embeddings.shape[1]\n","        self.index = faiss.IndexFlatL2(dim)\n","        self.index.add(embeddings)\n","\n","        # Build separate indices for each document type\n","        doc_types = set(chunk.doc_type for chunk in chunks_metadata)\n","        for doc_type in doc_types:\n","            type_indices = [i for i, chunk in enumerate(chunks_metadata)\n","                          if chunk.doc_type == doc_type]\n","            if type_indices:\n","                type_embeddings = embeddings[type_indices]\n","                type_index = faiss.IndexFlatL2(dim)\n","                type_index.add(type_embeddings)\n","                self.doc_type_indices[doc_type] = {\n","                    'index': type_index,\n","                    'mapping': type_indices  # Maps back to original chunks\n","                }\n","\n","        print(f\"✅ Indexed {len(chunks_metadata)} chunks across {len(doc_types)} document types\")\n","\n","    def retrieve(self, query: str, k: int = 4,\n","                filter_doc_type: Optional[str] = None,\n","                auto_route: bool = True) -> List[Tuple[ChunkMetadata, float]]:\n","        \"\"\"\n","        Retrieve relevant chunks with optional filtering and routing.\n","        Returns chunks with relevance scores.\n","        \"\"\"\n","        query_embedding = embed_model.encode([query])\n","\n","        # Determine which index to search\n","        if filter_doc_type and filter_doc_type in self.doc_type_indices:\n","            # Use filtered index\n","            type_data = self.doc_type_indices[filter_doc_type]\n","            D, I = type_data['index'].search(query_embedding, k)\n","            # Map back to original chunks\n","            chunk_indices = [type_data['mapping'][i] for i in I[0]]\n","            distances = D[0]\n","        elif auto_route:\n","            # Predict best document type\n","            predicted_type, confidence = predict_query_document_type(query)\n","            print(f\"🎯 Query routed to: {predicted_type} (confidence: {confidence:.2f})\")\n","\n","            if confidence > 0.7 and predicted_type in self.doc_type_indices:\n","                # High confidence - use specific index\n","                type_data = self.doc_type_indices[predicted_type]\n","                D, I = type_data['index'].search(query_embedding, k)\n","                chunk_indices = [type_data['mapping'][i] for i in I[0]]\n","                distances = D[0]\n","            else:\n","                # Low confidence - search all\n","                D, I = self.index.search(query_embedding, k)\n","                chunk_indices = I[0]\n","                distances = D[0]\n","        else:\n","            # Search all chunks\n","            D, I = self.index.search(query_embedding, k)\n","            chunk_indices = I[0]\n","            distances = D[0]\n","\n","        # Convert distances to similarity scores (inverse)\n","        max_dist = max(distances) if len(distances) > 0 else 1.0\n","        scores = [(max_dist - d) / max_dist for d in distances]\n","\n","        results = [(self.chunks_metadata[i], scores[idx])\n","                  for idx, i in enumerate(chunk_indices)]\n","\n","        return results"],"metadata":{"id":"Tqd7HSzPLMVJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 💬 Enhanced Answer Generation with Source Attribution"],"metadata":{"id":"fT4Dws--LN2F"}},{"cell_type":"code","source":["def generate_answer_with_sources(query: str,\n","                                retrieved_chunks: List[Tuple[ChunkMetadata, float]]) -> Dict:\n","    \"\"\"\n","    Generate answer with detailed source attribution.\n","    \"\"\"\n","    if not retrieved_chunks:\n","        return {\n","            'answer': \"I couldn't find relevant information to answer your question.\",\n","            'sources': [],\n","            'confidence': 0.0\n","        }\n","\n","    # Prepare context from retrieved chunks\n","    context_parts = []\n","    sources = []\n","\n","    for chunk_meta, score in retrieved_chunks:\n","        context_parts.append(f\"[From {chunk_meta.doc_type}, Pages {chunk_meta.page_start}-{chunk_meta.page_end}]\")\n","        context_parts.append(chunk_meta.text)\n","        context_parts.append(\"\")\n","\n","        sources.append({\n","            'doc_type': chunk_meta.doc_type,\n","            'pages': f\"{chunk_meta.page_start}-{chunk_meta.page_end}\",\n","            'relevance': f\"{score:.2%}\",\n","            'preview': chunk_meta.text[:100] + \"...\"\n","        })\n","\n","    context = \"\\n\".join(context_parts)\n","\n","    # Generate answer\n","    prompt = f\"\"\"\n","    You are a helpful AI assistant. Use the provided context to answer the question.\n","    Be specific and cite which document type and pages support your answer.\n","\n","    Context:\n","    {context}\n","\n","    Question: {query}\n","\n","    Instructions:\n","    1. Answer based ONLY on the provided context\n","    2. Mention which document type(s) contain the information\n","    3. Be concise but complete\n","    4. If the context doesn't contain enough information, say so\n","\n","    Answer:\n","    \"\"\"\n","\n","    try:\n","        response = gemini_model.generate_content(prompt)\n","        answer = response.text.strip()\n","\n","        # Calculate overall confidence based on retrieval scores\n","        avg_score = sum(s for _, s in retrieved_chunks) / len(retrieved_chunks)\n","\n","        return {\n","            'answer': answer,\n","            'sources': sources,\n","            'confidence': avg_score,\n","            'chunks_used': len(retrieved_chunks)\n","        }\n","    except Exception as e:\n","        print(f\"Answer generation error: {e}\")\n","        return {\n","            'answer': f\"Error generating answer: {str(e)}\",\n","            'sources': sources,\n","            'confidence': 0.0\n","        }"],"metadata":{"id":"IITpvhTeLRsg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 🏗️ Enhanced Document Store"],"metadata":{"id":"VbvV4zsCLTEu"}},{"cell_type":"code","source":["class EnhancedDocumentStore:\n","    \"\"\"\n","    Manages the complete document processing and retrieval pipeline.\n","    \"\"\"\n","\n","    def __init__(self):\n","        self.pages_info = []\n","        self.logical_docs = []\n","        self.chunks_metadata = []\n","        self.retriever = IntelligentRetriever()\n","        self.is_ready = False\n","        self.processing_stats = {}\n","        self.filename = None\n","\n","    def process_pdf(self, pdf_file, filename: str = \"document.pdf\"):\n","        \"\"\"\n","        Complete PDF processing pipeline.\n","        \"\"\"\n","        self.filename = filename\n","        self.is_ready = False\n","        start_time = datetime.now()\n","\n","        try:\n","            # Extract and analyze PDF\n","            self.pages_info, self.logical_docs = extract_and_analyze_pdf(pdf_file)\n","\n","            # Chunk documents with metadata\n","            self.chunks_metadata = process_all_documents(self.logical_docs)\n","\n","            # Build retrieval indices\n","            self.retriever.build_indices(self.chunks_metadata)\n","\n","            # Calculate processing statistics\n","            process_time = (datetime.now() - start_time).total_seconds()\n","            self.processing_stats = {\n","                'filename': filename,\n","                'total_pages': len(self.pages_info),\n","                'documents_found': len(self.logical_docs),\n","                'total_chunks': len(self.chunks_metadata),\n","                'document_types': list(set(doc.doc_type for doc in self.logical_docs)),\n","                'processing_time': f\"{process_time:.1f}s\"\n","            }\n","\n","            self.is_ready = True\n","            return True, self.processing_stats\n","\n","        except Exception as e:\n","            return False, {'error': str(e)}\n","\n","    def query(self, question: str, filter_type: Optional[str] = None,\n","             auto_route: bool = True, k: int = 4) -> Dict:\n","        \"\"\"\n","        Query the document store.\n","        \"\"\"\n","        if not self.is_ready:\n","            return {\n","                'answer': \"Please upload and process a PDF first.\",\n","                'sources': [],\n","                'confidence': 0.0\n","            }\n","\n","        # Retrieve relevant chunks\n","        retrieved = self.retriever.retrieve(\n","            question, k=k,\n","            filter_doc_type=filter_type,\n","            auto_route=auto_route\n","        )\n","\n","        # Generate answer with sources\n","        result = generate_answer_with_sources(question, retrieved)\n","        result['filter_used'] = filter_type or ('auto' if auto_route else 'none')\n","\n","        return result\n","\n","    def get_document_structure(self) -> List[Dict]:\n","        \"\"\"\n","        Get the document structure for UI display.\n","        \"\"\"\n","        if not self.logical_docs:\n","            return []\n","\n","        structure = []\n","        for doc in self.logical_docs:\n","            structure.append({\n","                'id': doc.doc_id,\n","                'type': doc.doc_type,\n","                'pages': f\"{doc.page_start + 1}-{doc.page_end + 1}\",  # 1-indexed for UI\n","                'chunks': len(doc.chunks) if doc.chunks else 0,\n","                'preview': doc.text[:200] + \"...\" if len(doc.text) > 200 else doc.text\n","            })\n","\n","        return structure"],"metadata":{"id":"lyy5OLSTLViM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 🎨 Gradio Interface with Enhanced Features\n","Now let's create the sophisticated Gradio interface:"],"metadata":{"id":"sbeCC7ItLWpA"}},{"cell_type":"code","source":["# Global store instance\n","doc_store = EnhancedDocumentStore()\n","\n","def process_pdf_handler(pdf_file):\n","    \"\"\"Handle PDF upload and processing.\"\"\"\n","    if pdf_file is None:\n","        return \"⚠️ Please upload a PDF file\", None, gr.update(choices=[\"All\"])\n","\n","    # Process the PDF\n","    success, stats = doc_store.process_pdf(pdf_file,\n","                                          filename=getattr(pdf_file, 'name', 'document.pdf'))\n","\n","    if success:\n","        # Prepare status message\n","        status_msg = f\"\"\"\n","        ✅ **Successfully Processed:**\n","        - 📄 File: {stats['filename']}\n","        - 📑 Pages: {stats['total_pages']}\n","        - 📚 Documents Found: {stats['documents_found']}\n","        - 🧩 Chunks Created: {stats['total_chunks']}\n","        - 🏷️ Types: {', '.join(stats['document_types'])}\n","        - ⏱️ Time: {stats['processing_time']}\n","        \"\"\"\n","\n","        # Get document structure for display\n","        structure = doc_store.get_document_structure()\n","        structure_display = \"\\n\".join([\n","            f\"• **{doc['type']}** (Pages {doc['pages']}): {doc['chunks']} chunks\"\n","            for doc in structure\n","        ])\n","\n","        # Update filter choices\n","        doc_types = [\"All\"] + stats['document_types']\n","\n","        return status_msg, structure_display, gr.update(choices=doc_types, value=\"All\")\n","    else:\n","        return f\"❌ Error: {stats.get('error', 'Unknown error')}\", None, gr.update(choices=[\"All\"])\n","\n","def chat_handler(message, history, doc_filter, auto_route, num_chunks):\n","    \"\"\"Handle chat interactions.\"\"\"\n","    if not doc_store.is_ready:\n","        response = \"📚 Please upload and process a PDF document first.\"\n","        return history + [[message, response]]\n","\n","    # Query the document store\n","    filter_type = None if doc_filter == \"All\" else doc_filter\n","    result = doc_store.query(\n","        message,\n","        filter_type=filter_type,\n","        auto_route=auto_route and filter_type is None,\n","        k=num_chunks\n","    )\n","\n","    # Format response with sources\n","    response = f\"{result['answer']}\\n\\n\"\n","\n","    if result['sources']:\n","        response += \"📍 **Sources:**\\n\"\n","        for src in result['sources']:\n","            response += f\"• {src['doc_type']} (Pages {src['pages']}) - Relevance: {src['relevance']}\\n\"\n","\n","    response += f\"\\n*Confidence: {result['confidence']:.1%} | Filter: {result['filter_used']}*\"\n","\n","    return history + [[message, response]]\n","\n","def create_interface():\n","    \"\"\"Create the enhanced Gradio interface with unified single-tab layout.\"\"\"\n","\n","    with gr.Blocks(title=\"Enhanced Document Q&A\", theme=gr.themes.Soft()) as demo:\n","        gr.Markdown(\"\"\"\n","        # 🚀 Enhanced Document Q&A System\n","        ### Intelligent Multi-Document Analysis with Advanced RAG Pipeline\n","        \"\"\")\n","\n","        with gr.Row():\n","            # Left side - PDF preview and upload\n","            with gr.Column(scale=2):\n","                pdf_input = PDF(\n","                    label=\"📄 PDF Document Viewer\",\n","                    interactive=True,\n","                    height=600\n","                )\n","\n","                with gr.Row():\n","                    process_btn = gr.Button(\n","                        \"🔄 Process Document\",\n","                        variant=\"primary\",\n","                        size=\"lg\",\n","                        scale=2\n","                    )\n","                    clear_all_btn = gr.Button(\n","                        \"🗑️ Clear All\",\n","                        variant=\"secondary\",\n","                        size=\"lg\",\n","                        scale=1\n","                    )\n","\n","            # Middle - Document info and settings\n","            with gr.Column(scale=1):\n","                gr.Markdown(\"### 📊 Document Info\")\n","                status_output = gr.Markdown(\n","                    value=\"⏳ Waiting for PDF upload...\"\n","                )\n","\n","                structure_output = gr.Markdown(\n","                    value=\"\",\n","                    label=\"Document Structure\"\n","                )\n","\n","                gr.Markdown(\"### ⚙️ Settings\")\n","\n","                doc_filter = gr.Dropdown(\n","                    choices=[\"All\"],\n","                    value=\"All\",\n","                    label=\"🏷️ Document Type Filter\",\n","                    info=\"Filter search to specific document type\"\n","                )\n","\n","                auto_route = gr.Checkbox(\n","                    value=True,\n","                    label=\"🎯 Auto-Route Queries\",\n","                    info=\"Automatically detect relevant document type\"\n","                )\n","\n","                num_chunks = gr.Slider(\n","                    minimum=1,\n","                    maximum=10,\n","                    value=4,\n","                    step=1,\n","                    label=\"📊 Chunks to Retrieve\"\n","                )\n","\n","            # Right side - Chat interface\n","            with gr.Column(scale=2):\n","                gr.Markdown(\"### 💬 Ask Questions\")\n","                chatbot = gr.Chatbot(\n","                    label=\"Conversation\",\n","                    height=500,\n","                    elem_id=\"chatbot\",\n","                    show_label=False\n","                )\n","\n","                with gr.Row():\n","                    msg_input = gr.Textbox(\n","                        label=\"Ask a question\",\n","                        placeholder=\"e.g., What are the payment terms? What is the total amount?\",\n","                        scale=4,\n","                        show_label=False\n","                    )\n","                    send_btn = gr.Button(\"📤 Send\", scale=1, variant=\"primary\")\n","\n","                with gr.Row():\n","                    clear_chat_btn = gr.Button(\"🗑️ Clear Chat\", size=\"sm\", scale=1)\n","                    example_btn1 = gr.Button(\"📝 What's the summary?\", size=\"sm\", scale=1)\n","                    example_btn2 = gr.Button(\"💰 Find amounts\", size=\"sm\", scale=1)\n","\n","        # Status bar at the bottom\n","        with gr.Row():\n","            status_bar = gr.Markdown(\n","                value=\"**Status:** Ready | **Documents:** 0 | **Chunks:** 0 | **Cache Hits:** 0/0\",\n","                elem_id=\"status_bar\"\n","            )\n","\n","        # Event handlers\n","        def update_status_bar():\n","            \"\"\"Update the status bar with current statistics.\"\"\"\n","            if doc_store.is_ready:\n","                stats = doc_store.processing_stats\n","                cache_rate = 0\n","                if hasattr(doc_store.retriever, 'total_queries') and doc_store.retriever.total_queries > 0:\n","                    cache_rate = (doc_store.retriever.cache_hits / doc_store.retriever.total_queries) * 100\n","\n","                return f\"**Status:** ✅ Ready | **Documents:** {stats.get('documents_found', 0)} | **Chunks:** {stats.get('total_chunks', 0)} | **Cache Rate:** {cache_rate:.0f}%\"\n","            return \"**Status:** Ready | **Documents:** 0 | **Chunks:** 0 | **Cache Hits:** 0/0\"\n","\n","        def clear_all():\n","            \"\"\"Clear everything and reset the interface.\"\"\"\n","            global doc_store\n","            doc_store = EnhancedDocumentStore()\n","            return (\n","                None,  # pdf_input\n","                \"⏳ Waiting for PDF upload...\",  # status_output\n","                \"\",  # structure_output\n","                gr.update(choices=[\"All\"], value=\"All\"),  # doc_filter\n","                [],  # chatbot\n","                \"\",  # msg_input\n","                update_status_bar()  # status_bar\n","            )\n","\n","        # Process PDF handler with status bar update\n","        def process_pdf_with_status(pdf_file):\n","            status, structure, filter_update = process_pdf_handler(pdf_file)\n","            status_bar_text = update_status_bar()\n","            return status, structure, filter_update, status_bar_text\n","\n","        # Chat handler with status bar update\n","        def chat_with_status(message, history, doc_filter, auto_route, num_chunks):\n","            new_history = chat_handler(message, history, doc_filter, auto_route, num_chunks)\n","            status_bar_text = update_status_bar()\n","            return new_history, status_bar_text\n","\n","        # Example question handlers\n","        def ask_summary(history):\n","            return chat_handler(\"Can you provide a summary of the main points in this document?\",\n","                              history, doc_filter.value, auto_route.value, num_chunks.value)\n","\n","        def ask_amounts(history):\n","            return chat_handler(\"What are all the monetary amounts or financial figures mentioned?\",\n","                              history, doc_filter.value, auto_route.value, num_chunks.value)\n","\n","        # Wire up all the events\n","        process_btn.click(\n","            fn=process_pdf_with_status,\n","            inputs=[pdf_input],\n","            outputs=[status_output, structure_output, doc_filter, status_bar]\n","        )\n","\n","        clear_all_btn.click(\n","            fn=clear_all,\n","            outputs=[pdf_input, status_output, structure_output, doc_filter,\n","                    chatbot, msg_input, status_bar]\n","        )\n","\n","        # Chat interactions\n","        msg_input.submit(\n","            fn=chat_with_status,\n","            inputs=[msg_input, chatbot, doc_filter, auto_route, num_chunks],\n","            outputs=[chatbot, status_bar]\n","        ).then(\n","            lambda: \"\",\n","            outputs=[msg_input]\n","        )\n","\n","        send_btn.click(\n","            fn=chat_with_status,\n","            inputs=[msg_input, chatbot, doc_filter, auto_route, num_chunks],\n","            outputs=[chatbot, status_bar]\n","        ).then(\n","            lambda: \"\",\n","            outputs=[msg_input]\n","        )\n","\n","        clear_chat_btn.click(\n","            lambda: [],\n","            outputs=[chatbot]\n","        )\n","\n","        example_btn1.click(\n","            fn=ask_summary,\n","            inputs=[chatbot],\n","            outputs=[chatbot]\n","        ).then(\n","            fn=update_status_bar,\n","            outputs=[status_bar]\n","        )\n","\n","        example_btn2.click(\n","            fn=ask_amounts,\n","            inputs=[chatbot],\n","            outputs=[chatbot]\n","        ).then(\n","            fn=update_status_bar,\n","            outputs=[status_bar]\n","        )\n","\n","        # Auto-process when PDF is uploaded\n","        pdf_input.change(\n","            fn=process_pdf_with_status,\n","            inputs=[pdf_input],\n","            outputs=[status_output, structure_output, doc_filter, status_bar]\n","        )\n","\n","    return demo"],"metadata":{"id":"k7wXGcnJN6rf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["demo = create_interface()\n","demo.launch(share=True, debug=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["c3aa2c6b6f7841989afb6a100b952afd","cc4d50a95b3f43e782f886dd5146aaf1","98057286293d42c9864daf18481f883c","d965fba23d3b44c992bdc3984749a7d2","3ce32b6cca37487580f4bc7e72df467e","7e595c8e8ef94f7385d09fa42459817b","97e38358ebb14978a1bf137acf408598","75e48517bae846c3bd1aab5f45adbf31","65f13cdcd8684079b5ac77bbf2f6c51f","09a806104a6f4409a9387c44d7ec4005","776db3a8e2c94292886fa8593e82820f","71929f3f2bff40469af3cced1a3682ea","c911b2c5b0874d0d905d50122e7fe5d6","49a6916f7e9b4a8a80217e4b91752e5e","d6965a0eceb24f17a860449cf55562ec","d6c4825c9e0e4a4c926b4b5cc4c51bc7","f59d4a8273e543ed81ae95a84f2510ff","205cb200b0e24214a09d218ffdf7760f","2eb534d1e5b84192b496c89f3ad027b7","e3b02ebba16742879d2309d29f66599b","ca62e0e2d37a48b9af2bdc46474d84cf","0626dfabfdff46c7851af8d2f8b24911","91f360da928b49c7bedca23bc5a8ac5b","fa089194963b4189b5eb0252a4bfa7f8","8ce86fdcf2724c32a01f9248402b4bdb","718dde41bfb447868427ac9cc91dd13c","36cc5c055bc34dc1adbab662cd0b814d","92273b9a5eba4677bc0431c034903b01","68aea2b85a164fdea012fdc3d3313a21","1ba23b9983e849a498027a6e551f43a4","523f5f5435594ebd9e8468de2ed4eb34","dc3293f05c11409992eaa7add37f5d5c","4b1a05dfb0944507be4d4a2fc7d7688c"]},"id":"LJJxZwTpN_pQ","executionInfo":{"status":"ok","timestamp":1754444934010,"user_tz":240,"elapsed":12704979,"user":{"displayName":"Chaitanya Baweja","userId":"09943888125622581847"}},"outputId":"29c9e9d3-745e-4678-b202-02166e4b3ff4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3480370060.py:136: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n","  chatbot = gr.Chatbot(\n"]},{"output_type":"stream","name":"stdout","text":["Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","* Running on public URL: https://be54e9d8c68188a478.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://be54e9d8c68188a478.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["📖 Starting PDF extraction and analysis...\n","✅ Extracted 4 pages\n","🧠 Analyzing document structure...\n","  Page 0: New document detected - Resume\n","  Page 1: New document detected - Lender Fee Sheet\n","  Page 2: New document detected - Pay Slip\n","  Page 3: New document detected - Pay Slip\n","✅ Identified 4 logical documents\n","   - Resume: Pages 0-0\n","   - Lender Fee Sheet: Pages 1-1\n","   - Pay Slip: Pages 2-2\n","   - Pay Slip: Pages 3-3\n","📄 Resume: Created 1 chunks\n","📄 Lender Fee Sheet: Created 1 chunks\n","📄 Pay Slip: Created 1 chunks\n","📄 Pay Slip: Created 1 chunks\n","🔨 Building vector indices...\n"]},{"output_type":"display_data","data":{"text/plain":["Batches:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c3aa2c6b6f7841989afb6a100b952afd"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n","  return forward_call(*args, **kwargs)\n"]},{"output_type":"stream","name":"stdout","text":["✅ Indexed 4 chunks across 3 document types\n","Query routing error: Expecting value: line 1 column 1 (char 0)\n","🎯 Query routed to: Other (confidence: 0.00)\n","Query routing error: Expecting value: line 1 column 1 (char 0)\n","🎯 Query routed to: Other (confidence: 0.00)\n","📖 Starting PDF extraction and analysis...\n","✅ Extracted 7 pages\n","🧠 Analyzing document structure...\n","  Page 0: New document detected - Lender Fee Sheet\n","  Page 1: New document detected - Pay Slip\n","  Page 2: New document detected - Contract\n","  Page 4: New document detected - Contract\n","  Page 5: New document detected - Contract\n","  Page 6: New document detected - Form\n","✅ Identified 6 logical documents\n","   - Lender Fee Sheet: Pages 0-0\n","   - Pay Slip: Pages 1-1\n","   - Contract: Pages 2-3\n","   - Contract: Pages 4-4\n","   - Contract: Pages 5-5\n","   - Form: Pages 6-6\n","📄 Lender Fee Sheet: Created 1 chunks\n","📄 Pay Slip: Created 1 chunks\n","📄 Contract: Created 2 chunks\n","📄 Contract: Created 1 chunks\n","📄 Contract: Created 1 chunks\n","📄 Form: Created 1 chunks\n","🔨 Building vector indices...\n"]},{"output_type":"display_data","data":{"text/plain":["Batches:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"71929f3f2bff40469af3cced1a3682ea"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["✅ Indexed 7 chunks across 4 document types\n","📖 Starting PDF extraction and analysis...\n","✅ Extracted 4 pages\n","🧠 Analyzing document structure...\n","  Page 0: New document detected - Resume\n","  Page 1: New document detected - Lender Fee Sheet\n","  Page 2: New document detected - Pay Slip\n","  Page 3: New document detected - Pay Slip\n","✅ Identified 4 logical documents\n","   - Resume: Pages 0-0\n","   - Lender Fee Sheet: Pages 1-1\n","   - Pay Slip: Pages 2-2\n","   - Pay Slip: Pages 3-3\n","📄 Resume: Created 1 chunks\n","📄 Lender Fee Sheet: Created 1 chunks\n","📄 Pay Slip: Created 1 chunks\n","📄 Pay Slip: Created 1 chunks\n","🔨 Building vector indices...\n"]},{"output_type":"display_data","data":{"text/plain":["Batches:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91f360da928b49c7bedca23bc5a8ac5b"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["✅ Indexed 4 chunks across 3 document types\n","Query routing error: Expecting value: line 1 column 1 (char 0)\n","🎯 Query routed to: Other (confidence: 0.00)\n","Keyboard interruption in main thread... closing server.\n","Killing tunnel 127.0.0.1:7860 <> https://be54e9d8c68188a478.gradio.live\n"]},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":36}]}]}