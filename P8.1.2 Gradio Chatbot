{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":680},"executionInfo":{"elapsed":17869,"status":"ok","timestamp":1757995940047,"user":{"displayName":"Dfe Shah","userId":"02290639415740118939"},"user_tz":-330},"id":"tUm4s5QgKeXm","outputId":"2f54c1b3-d6d4-4131-a071-5815f7700285"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipython-input-1228468710.py:13: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n","  chatbot = gr.Chatbot(label=\"Chat History\", height=300)\n"]},{"name":"stdout","output_type":"stream","text":["It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://fd1cd5783c05ce7598.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"data":{"text/html":["\u003cdiv\u003e\u003ciframe src=\"https://fd1cd5783c05ce7598.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\u003c/div\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":[]},"execution_count":1,"metadata":{},"output_type":"execute_result"}],"source":["import gradio as gr\n","\n","def process_pdf(file):\n","    return \"PDF processed successfully!\"\n","\n","def handle_chat(message, history):\n","    return history + [(message, \"This is a mock answer.\")]\n","\n","with gr.Blocks(title=\"Step 5: Full Functioning UI\") as demo:\n","    gr.Markdown(\"### Step 5: Connected UI\")\n","\n","    with gr.Column(scale=2):\n","        chatbot = gr.Chatbot(label=\"Chat History\", height=300)\n","        user_input = gr.Textbox(\n","            placeholder=\"Ask a question about your document...\",\n","            label=\"Your Question\"\n","        )\n","        send_btn = gr.Button(\"ğŸ“¤ Send\")\n","        clear_btn = gr.Button(\"ğŸ—‘ï¸ Clear Chat\")\n","\n","    with gr.Column(scale=1):\n","        pdf_input = gr.File(label=\"ğŸ“„ Upload PDF\", file_types=[\".pdf\"])\n","        process_btn = gr.Button(\"ğŸ”„ Process Document\")\n","\n","    process_btn.click(process_pdf, inputs=pdf_input, outputs=None)\n","    send_btn.click(handle_chat, inputs=[user_input, chatbot], outputs=chatbot)\n","    user_input.submit(handle_chat, inputs=[user_input, chatbot], outputs=chatbot)\n","    clear_btn.click(lambda: [], outputs=chatbot)\n","\n","demo.launch()\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":518,"status":"ok","timestamp":1757996529670,"user":{"displayName":"Dfe Shah","userId":"02290639415740118939"},"user_tz":-330},"id":"Pi_XPxtBj5Sl","outputId":"32f85b5f-63a4-40dc-a193-90d9d8efaa72"},"outputs":[{"name":"stderr","output_type":"stream","text":["/tmp/ipython-input-1390855550.py:45: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n","  chatbot = gr.Chatbot(\n","/tmp/ipython-input-1390855550.py:45: DeprecationWarning: The 'bubble_full_width' parameter is deprecated and will be removed in a future version. This parameter no longer has any effect.\n","  chatbot = gr.Chatbot(\n"]}],"source":["# --- Add these functions to your backend logic ---\n","import json\n","import gradio as gr\n","\n","def save_chat(chat_history):\n","    \"\"\"Saves the chat history to a JSON file and returns the file path.\"\"\"\n","    if not chat_history:\n","        return None # Nothing to save\n","\n","    # Save to a temporary file\n","    file_path = \"chat_history.json\"\n","    with open(file_path, 'w') as f:\n","        json.dump(chat_history, f)\n","\n","    return file_path\n","\n","def load_chat(file):\n","    \"\"\"Loads chat history from an uploaded JSON file.\"\"\"\n","    if file is None:\n","        return None\n","\n","    with open(file.name, 'r') as f:\n","        chat_history = json.load(f)\n","    return chat_history\n","\n","# --- MODIFIED Gradio Interface ---\n","\n","with gr.Blocks(title=\"PDF Chatbot\") as app:\n","    gr.Markdown(\"# ğŸ“„ PDF Research Buddy\")\n","    gr.Markdown(\"Upload your PDF and ask any questions you have about its content.\")\n","\n","    with gr.Row():\n","        with gr.Column(scale=1):\n","            file_uploader = gr.UploadButton(\"ğŸ“¤ Upload PDF\", file_types=[\".pdf\"])\n","            status_box = gr.Textbox(label=\"Status\", value=\"Please upload a PDF to get started.\", interactive=False)\n","\n","            ## NEW: Add Save/Load buttons and a File component for download ##\n","            with gr.Row():\n","                save_btn = gr.Button(\"ğŸ’¾ Save Chat\")\n","                load_btn = gr.UploadButton(\"ğŸ“‚ Load Chat\", file_types=[\".json\"])\n","\n","            download_file = gr.File(label=\"Download Chat History\", visible=False)\n","\n","        with gr.Column(scale=2):\n","            chatbot = gr.Chatbot(\n","                label=\"Chat\",\n","                bubble_full_width=False,\n","                avatar_images=(None, \"https://seeklogo.com/images/G/google-gemini-logo-2462199324-seeklogo.com.png\")\n","            )\n","            msg_box = gr.Textbox(label=\"Your Question\", placeholder=\"Type your question here and press Enter...\")\n","\n","    # --- MODIFIED Event Handlers ---\n","    # file_uploader.upload(create_rag_pipeline, inputs=file_uploader, outputs=status_box) # Commented out as create_rag_pipeline is not defined\n","    # msg_box.submit(ask_question, inputs=[msg_box, chatbot], outputs=chatbot) # Commented out as ask_question is not defined\n","    msg_box.submit(lambda: \"\", inputs=None, outputs=msg_box)\n","\n","    ## NEW: Wire up the save and load buttons ##\n","    save_btn.click(save_chat, inputs=chatbot, outputs=download_file).then(\n","        lambda: gr.update(visible=True), outputs=download_file\n","    )\n","    load_btn.upload(load_chat, inputs=load_btn, outputs=chatbot)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":0},"id":"Rn-qW3JaweCR"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m310.5/310.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m75.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","ipython 7.34.0 requires jedi\u003e=0.16, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mColab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n","* Running on public URL: https://fd4bfae0e0f0780d83.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"data":{"text/html":["\u003cdiv\u003e\u003ciframe src=\"https://fd4bfae0e0f0780d83.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen\u003e\u003c/iframe\u003e\u003c/div\u003e"],"text/plain":["\u003cIPython.core.display.HTML object\u003e"]},"metadata":{},"output_type":"display_data"}],"source":["# --- 1. Installation ---\n","# Install all required libraries\n","!pip install gradio llama-index pypdf llama-index-llms-gemini -q\n","\n","# --- 2. Imports ---\n","import gradio as gr\n","import os\n","import json\n","from google.colab import userdata\n","from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, Document\n","from llama_index.llms.gemini import Gemini\n","from llama_index.core.memory import ChatMemoryBuffer\n","\n","# --- 3. Setup ---\n","# Configure the Gemini API Key\n","# IMPORTANT: Make sure you have a secret in Colab named \"GOOGLE_API_KEY\" with your key.\n","os.environ[\"GOOGLE_API_KEY\"] = userdata.get(\"GOOGLE_API_KEY\")\n","\n","# --- 4. Backend RAG and Chat Logic ---\n","\n","# Use a global variable to hold the query engine to maintain state\n","query_engine = None\n","\n","def create_rag_pipeline(files: list):\n","    \"\"\"\n","    Creates the RAG query engine from a list of uploaded files.\n","    This function implements the Multi-File Q\u0026A feature.\n","    \"\"\"\n","    global query_engine\n","    if not files:\n","        return \"Please upload at least one PDF to begin.\"\n","\n","    # Create a temporary directory to store uploaded files\n","    os.makedirs(\"temp_docs\", exist_ok=True)\n","\n","    # Clear old files from the directory\n","    for f in os.listdir(\"temp_docs\"):\n","        os.remove(os.path.join(\"temp_docs\", f))\n","\n","    file_names = []\n","    # Save all uploaded files to the temporary directory\n","    for file in files:\n","        file_path = file.name\n","        temp_file_path = os.path.join(\"temp_docs\", os.path.basename(file_path))\n","\n","        with open(temp_file_path, 'wb') as f:\n","            # Gradio's file object has a .read() method if it's an UploadFile\n","            # but is just a path if it's a temp file. We handle both.\n","            content = file.read()\n","            f.write(content)\n","        file_names.append(os.path.basename(file_path))\n","\n","    # Load all documents from the directory\n","    reader = SimpleDirectoryReader(input_dir=\"temp_docs\")\n","    documents = reader.load_data()\n","\n","    # Configure LlamaIndex settings\n","    Settings.llm = Gemini(model=\"models/gemini-pro\")\n","\n","    # Create the vector store index\n","    index = VectorStoreIndex.from_documents(documents)\n","\n","    # Create a chat memory buffer\n","    memory = ChatMemoryBuffer.from_defaults(token_limit=3900)\n","\n","    # Create the chat engine\n","    query_engine = index.as_chat_engine(\n","        chat_mode=\"context\",\n","        memory=memory,\n","        system_prompt=\"You are a helpful and expert assistant. Please answer the user's questions based on the provided documents.\"\n","    )\n","\n","    return f\"âœ… Ready to answer questions from: {', '.join(file_names)}\"\n","\n","def ask_question(message: str, chat_history: list):\n","    \"\"\"\n","    Handles the user's question, gets a response from the RAG pipeline,\n","    and formats the sources for display. This implements the Source Highlighting feature.\n","    \"\"\"\n","    global query_engine\n","    if query_engine is None:\n","        return \"Please upload a PDF file first to begin.\", None\n","\n","    # Use .query() to get both the response and the source nodes\n","    response = query_engine.query(message)\n","\n","    # Format the source nodes into a readable markdown string\n","    source_text = \"--- SOURCES ---\\n\"\n","    for i, node in enumerate(response.source_nodes):\n","        source_text += f\"**Source {i+1} (Confidence: {node.score:.2f})**:\\n\"\n","        source_text += f\"```\\n{node.get_text().strip()}\\n```\\n\\n\"\n","\n","    # Update chat history manually\n","    chat_history.append((message, response.response))\n","\n","    return chat_history, source_text\n","\n","def save_chat(chat_history: list):\n","    \"\"\"\n","    Saves the current chat history to a JSON file.\n","    This implements the Save History feature.\n","    \"\"\"\n","    if not chat_history:\n","        return None\n","\n","    file_path = \"chat_history.json\"\n","    with open(file_path, 'w', encoding='utf-8') as f:\n","        json.dump(chat_history, f, ensure_ascii=False, indent=2)\n","\n","    return file_path\n","\n","def load_chat(file: gr.File):\n","    \"\"\"\n","    Loads chat history from an uploaded JSON file.\n","    This implements the Load History feature.\n","    \"\"\"\n","    if file is None:\n","        return None\n","\n","    with open(file.name, 'r', encoding='utf-8') as f:\n","        chat_history = json.load(f)\n","    return chat_history\n","\n","# --- 5. Gradio User Interface ---\n","\n","# Use a modern theme for the UI\n","theme = gr.themes.Soft(primary_hue=\"blue\", secondary_hue=\"sky\")\n","\n","with gr.Blocks(theme=theme, title=\"Advanced PDF Chatbot\") as app:\n","    gr.Markdown(\"# ğŸ¤– Advanced PDF Chatbot\")\n","    gr.Markdown(\"Upload one or more PDFs, ask questions, and see the sources for each answer.\")\n","\n","    with gr.Row():\n","        with gr.Column(scale=1):\n","            # Feature 2: Multi-File Q\u0026A\n","            file_uploader = gr.File(\n","                label=\"ğŸ“¤ Upload Your PDFs\",\n","                file_count=\"multiple\",\n","                file_types=[\".pdf\"]\n","            )\n","            status_box = gr.Textbox(label=\"Status\", value=\"Please upload PDF(s) to get started.\", interactive=False)\n","\n","            # Feature 1: Save and Load Chat History\n","            with gr.Row():\n","                save_btn = gr.Button(\"ğŸ’¾ Save Chat\")\n","                load_btn = gr.UploadButton(\"ğŸ“‚ Load Chat\", file_types=[\".json\"])\n","            download_file = gr.File(label=\"Download Chat History\", visible=False)\n","\n","            # Feature 3: Source Highlighting\n","            source_display = gr.Markdown(label=\"Sources Used for Answer\")\n","\n","        with gr.Column(scale=2):\n","            # Corrected Chatbot component with type='messages'\n","            chatbot = gr.Chatbot(\n","                label=\"Chat\",\n","                type='messages',\n","                avatar_images=(None, \"https://seeklogo.com/images/G/google-gemini-logo-2462199324-seeklogo.com.png\")\n","            )\n","            msg_box = gr.Textbox(label=\"Your Question\", placeholder=\"Type your question here and press Enter...\", scale=7)\n","\n","    # --- 6. Event Handlers ---\n","\n","    # Handle file uploads to create the RAG pipeline\n","    file_uploader.upload(create_rag_pipeline, inputs=file_uploader, outputs=status_box)\n","\n","    # Handle user questions to update both the chatbot and source display\n","    msg_box.submit(\n","        ask_question,\n","        inputs=[msg_box, chatbot],\n","        outputs=[chatbot, source_display]\n","    )\n","    # Clear the message box after submit\n","    msg_box.submit(lambda: \"\", inputs=None, outputs=msg_box)\n","\n","    # Handle save/load functionality\n","    save_btn.click(save_chat, inputs=chatbot, outputs=download_file).then(\n","        lambda: gr.update(visible=True), outputs=download_file\n","    )\n","    load_btn.upload(load_chat, inputs=load_btn, outputs=chatbot)\n","\n","# --- 7. Launch the App ---\n","if __name__ == \"__main__\":\n","    app.launch(debug=True, share=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VsjgjXfdz12t"},"outputs":[],"source":[]}],"metadata":{"colab":{"name":"","provenance":[{"file_id":"1r-8GIiGXfQ8PT8T1GTtz35WxevYDXinq","timestamp":1757995884920},{"file_id":"1TtPgkS5PPaibPS_6fZdN2u-i3_gsSnM8","timestamp":1754620978438}],"version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}